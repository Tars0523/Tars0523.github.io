---
title: "Graduated Non-Convexity (1)"
date: 2025-10-01 13:00 +0900
categories: [Optimization]   
tags: [Robust Estimation]        
---

**Graduated Non-Convexity for Robust Spatial Perception: From Non-Minimal Solvers to Global Outlier Rejection**  
라는 논문에 대해서 리뷰해보려고 한다.

나에게는 좀 어려운 논문이어서 아직 잘 이해하지는 못했지만, 대략적으로는  
(i) Graduated Non-Convexity (GNC) 와  
(ii) Robust Estimation ≈ Outlier Process (Black-Rangarajan Duality)  

라는 두 개의 이론을 합친 논문 같다.

---

## 왜 이런 수학적 "주짓수"를 하는가?

그 이유는 **outlier에 강건**하면서도 **local optimal에 빠지지 않는 robust solver**를 만들고자 하는 것이다.

- (a) local optimal을 잘 피해갈 수 있는 방법  
- (b) robust solver가 필요한 이유  

---

## 왜 Robust Solver가 필요한가?

그 이유는 *관측 모델(observation model)* 이 불완전하기 때문이다.  
로보틱스에서 주로 다루는 문제는 다음과 같이 표현된다:

$$
\mathbf{y}_i = \mathbf{h}(\mathbf{x}) + \sigma_i
$$

$$
\min_{x} \sum_{i} \|\mathbf{y}_i - \mathbf{h}(\mathbf{x})\|. \tag{1}
$$

여기서  

- $\mathbf{y}$: 관측(observation)  
- $\mathbf{x}$: 추정하고자 하는 변수(variable)  
- $\sigma$: 노이즈(noise)  

관측은 neural network로 얻어낸 값일 수도 있고, 센서에서 나온 raw 데이터일 수도 있다.  
변수는 rotation, translation, scale 등 다양하다.  
노이즈는 isotropic, anisotropic Gaussian 등으로 모델링할 수 있다.

---

## 왜 Local Optimal을 피해야 하나?

현실에서는 모델링한 모델과 빗나가는 **outlier**가 존재한다.  
따라서 이러한 불확실성 때문에 (b) robust solver가 필요하다.

보통 outlier에 강건하게 만들기 위해서는 **robust kernel** $\rho(\cdot)$ 을 씌운다:

$$
\min_{x} \sum_{i} \rho\!\left(\|\mathbf{y}_i - \mathbf{h}(\mathbf{x})\|\right). \tag{2}
$$

여기서 robust kernel을 씌우는 것은 **확률 분포 가정**을 바꾸는 것과 동치이다.  
예를 들어:

- (1) 식은 Gaussian 분포 가정 (MLE).  
- 만약 Cauchy 분포를 가정한다면, (1)에 Cauchy kernel을 씌운 형태가 된다.  

Cauchy 분포는 heavy-tail 특성을 가지므로 outlier가 많은 상황에 robust하다.  
즉, **분포 가정이 바뀌면 robust estimation 효과가 자연스럽게 반영**된다.

---

## 문제점

- 식 (1)의 경우 제약 없는 비제약 최적화라면 global optimal을 **closed form**으로 얻을 수 있다.  
- 하지만 식 (2)처럼 robust kernel이 들어가면 **비선형**이 추가되어 global optimal을 구하기 어렵다.  
- 따라서 local optimal에 갇혀버리는 문제가 생긴다.  

그래서 (a) local optimal을 피해갈 수 있는 전략이 필요하다.

---

## 다음 단계

이제 배경 설명을 마쳤으니, 논문에서 제시하는 **자세한 수학적 유도**를 살펴보자.

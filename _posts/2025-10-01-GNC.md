---
title: "Graduated Non-Convexity"
date: 2025-10-01 13:00 +0900
categories: [Optimization]
tags: [Robust Estimation]
math: true
---

**Graduated Non-Convexity for Robust Spatial Perception: From Non-Minimal Solvers to Global Outlier Rejection**  
논문을 리뷰해보려 한다.

아직 완전히 이해한 것은 아니지만, 대략적으로는  
(i) **Graduated Non-Convexity (GNC)** 와  
(ii) **Robust Estimation ≈ Outlier Process (Black–Rangarajan Duality)**  
두 이론을 결합한 논문으로 보인다.

---

## 왜 이런 “수학-주짓수”를 하나?

목표는 **outlier에 강건**하면서도 **local optimum**에 갇히지 않는 **robust solver**를 만드는 것이다.

- (a) local optimum을 피하는 전략  
- (b) robust solver 자체의 필요성

---

## (b) Robust solver는 왜 필요한가?

관측 모델이 현실에서 **불완전**하기 때문이다. 로보틱스에서 자주 쓰는 기본 형태는 다음과 같다.

\[
\mathbf{y}_i = \mathbf{h}(\mathbf{x}) + \boldsymbol{\epsilon}_i,
\]
\[
\min_{\mathbf{x}} \sum_{i} \left\lVert \mathbf{y}_i - \mathbf{h}(\mathbf{x}) \right\rVert. \tag{1}
\]

여기서  
- \(\mathbf{y}_i\): 관측 (sensor raw 값 또는 neural network 출력 등)  
- \(\mathbf{x}\): 추정 변수 (rotation, translation, scale 등)  
- \(\boldsymbol{\epsilon}_i\): 노이즈 (isotropic / anisotropic Gaussian 등으로 모델링)

관측–변수–모델 \( \mathbf{h}(\cdot) \)이 이상적으로 잘 맞으면 (MLE 관점에서) \(\mathbf{x}\)를 찾으면 된다.  
하지만 현실에는 **outlier**가 존재하므로, 모델 불확실성 때문에 **robust solver**가 필요하다.

---

## (a) Local optimum을 왜 피해야 하나?

Outlier에 강건하게 만들기 위해 보통 **robust kernel** \(\rho(\cdot)\)을 도입한다:

\[
\min_{\mathbf{x}} \sum_{i} \rho\!\left( \left\lVert \mathbf{y}_i - \mathbf{h}(\mathbf{x}) \right\rVert \right). \tag{2}
\]

이는 **확률 분포 가정**을 바꾸는 것과 동치다.  
예를 들어 (1)은 Gaussian 가정의 MLE이고, Cauchy 분포를 가정하면 (1)에 **Cauchy kernel**을 씌운 (2) 형태가 된다.  
Cauchy는 **heavy-tail**이라 outlier가 많을 때 robust하다. 즉, **분포 가정 → 최적화의 robust 특성**으로 반영된다.

문제는 다음과 같다.

- 식 (1)은 제약이 없을 때 global optimum을 **closed form**으로 얻을 수 있는 경우가 많다.  
- 반면 식 (2)는 **비선형성**이 추가되어 global optimum을 구하기 어렵거나 불가능해지고,  
  그 결과 **local optimum**에 쉽게 갇힌다.  
→ 따라서 (a)를 피하는 전략이 필요하다.

---

## 다음

이제 배경을 정리했으니, 논문이 제시하는 **수학적 유도**를 구체적으로 살펴본다.

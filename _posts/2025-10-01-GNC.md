---
title: "Graduated Non-Convexity (1)"
date: 2025-10-01 13:00 +0900
categories: [Optimization]
tags: [Robust Estimation]
math: true
---

*Graduated Non-Convexity for Robust Spatial Perception: From Non-Minimal Solvers to Global Outlier Rejection* 논문을 리뷰해보려 한다.
아직 완전히 이해한 것은 아니지만, 대략적으로는 (i) *Graduated Non-Convexity (GNC)* 와 (ii) *Robust Estimation ≈ Outlier Process (Black–Rangarajan Duality)* 두 이론을 결합한 논문으로 보인다.
이 논문의 목표는 (a) *outlier에 강건* 하면서도 (b) *local optimum*에 갇히지 않는 *robust solver*를 만드는 것으로 보인다. 왜 (a) 와 (b) 가 중요할까?

관측 모델이 현실에서 *불완전*하기 때문이다. 로보틱스에서 자주 쓰는 목적 함수의 기본 형태는 다음과 같다:

$$
\min_{\mathbf{x}} \sum_{i} \left\lVert \mathbf{y}_i - \mathbf{h}(\mathbf{x}) \right\rVert. \tag{1}
$$

여기서 $\mathbf{y}_i$: 관측 (sensor raw 값 또는 neural network 출력 등), $\mathbf{x}$: 추정 변수 (rotation, translation, scale 등), 그리고 $\boldsymbol{\epsilon}_i$: 노이즈 (isotropic / anisotropic Gaussian 등으로 모델링) 이다. 관측–변수–모델 $ \mathbf{h}(\cdot) $이 이상적으로 잘 맞으면 $\mathbf{x}$를 찾으면 된다. 하지만 현실에는 모델링의 한계 때문에 *outlier*가 존재하므로, 모델 불확실성 때문에 (a)의 특성을 가진, *robust solver*가 필요하다.

Outlier에 강건하게 만들기 위해 보통 **robust kernel** $\rho(\cdot)$을 도입한다:

$$
\min_{\mathbf{x}} \sum_{i} \rho\!\left( \left\lVert \mathbf{y}_i - \mathbf{h}(\mathbf{x}) \right\rVert \right). \tag{2}
$$

이는 *확률 분포 가정*을 바꾸는 것과 동치다. 예를 들어 (1)은 *Gaussian Distribution* 가정에서의 *MLE (Maximum Likelihood Estimation)* 이고, 예를 들어 *Cauchy Distribution*을 가정하면 (1)에 $\rho(cdot)$ 을 씌운 (2) 형태가 된다.  
$\rho(cdot)$ 은 어떠한 분포를 가정하냐에 따라서, 달라진다. *Cauchy* 혹은 *student-t* 와 같은 *heavy-tail* 분포는, outlier가 많다고 가정하는 분포이기 때문에 (*heavy-tail*), outlier가 실제로 많을 때 robust하다. 즉, *분포 가정은 → 최적화의 robust 특성*으로 반영된다.
그러면 식 (2)를 풀면 되는거 아닌가? 라고 생각할 수 있지만, 문제는 다음과 같다. 식 (1)은 제약이 없을 때 global optimum을 *closed form*으로 얻을 수 있는 경우가 많다. 반면 식 (2)는 *비선형성, non-convexity*이 추가되어 global optimum을 구하기 어렵거나 불가능해지고, 그 결과 *local optimum*에 쉽게 갇힌다. 따라서 (b) *local optimum* 를 피하는 전략이 필요하다.

이제 배경을 정리했으니, 논문이 제시하는 *수학적 유도*를 구체적으로 살펴본다.